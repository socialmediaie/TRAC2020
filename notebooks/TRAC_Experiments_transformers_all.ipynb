{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRAC_Experiments_transformers_all.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuVTg8VLViNd",
        "colab_type": "code",
        "outputId": "71fa64b0-b535-4a98-dd16-29c2ff71994b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "%%bash\n",
        "pip install torch transformers tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (45.2.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpgcK6edVyt7",
        "colab_type": "code",
        "outputId": "c94b5162-540f-4703-aa6a-6a8973f8824a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKblDQmBV5ky",
        "colab_type": "code",
        "outputId": "67c140ce-903d-44a6-f3c7-80257f9078da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%env TRAC_PATH /content/gdrive/My Drive/TRAC2020-master/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: TRAC_PATH=/content/gdrive/My Drive/TRAC2020-master/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM-Kk2LHWD5J",
        "colab_type": "code",
        "outputId": "55caca4e-16df-4ffc-f869-3eb0665d7d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "%%bash \n",
        "echo \"${TRAC_PATH}\"\n",
        "ls -ltrh \"${TRAC_PATH}/data\"\n",
        "realpath \"${TRAC_PATH}\"\n",
        "ls -ltrh \"${TRAC_PATH}\"/data/**/*"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/TRAC2020-master/\n",
            "total 4.0K\n",
            "drwx------ 2 root root 4.0K Mar  9 08:49 raw\n",
            "/content/gdrive/My Drive/TRAC2020-master\n",
            "-rw------- 1 root root  321 Mar  9 08:49 /content/gdrive/My Drive/TRAC2020-master//data/raw/README.txt\n",
            "\n",
            "/content/gdrive/My Drive/TRAC2020-master//data/raw/iben:\n",
            "total 560K\n",
            "-rw------- 1 root root  82K Mar  9 08:49 trac2_iben_test.csv\n",
            "-rw------- 1 root root  92K Mar  9 08:49 trac2_iben_dev.csv\n",
            "-rw------- 1 root root 385K Mar  9 08:49 trac2_iben_train.csv\n",
            "-rw------- 1 root root 1.3K Mar  9 08:49 README.txt\n",
            "\n",
            "/content/gdrive/My Drive/TRAC2020-master//data/raw/hin:\n",
            "total 807K\n",
            "-rw------- 1 root root 1.3K Mar  9 08:49 README.txt\n",
            "-rw------- 1 root root 174K Mar  9 08:49 trac2_hin_test.csv\n",
            "-rw------- 1 root root 507K Mar  9 08:50 trac2_hin_train.csv\n",
            "-rw------- 1 root root 125K Mar  9 08:50 trac2_hin_dev.csv\n",
            "\n",
            "/content/gdrive/My Drive/TRAC2020-master//data/raw/eng:\n",
            "total 903K\n",
            "-rw------- 1 root root 495K Mar  9 08:49 trac2_eng_train.csv\n",
            "-rw------- 1 root root 281K Mar  9 08:49 trac2_eng_test.csv\n",
            "-rw------- 1 root root 126K Mar  9 08:50 trac2_eng_dev.csv\n",
            "-rw------- 1 root root 1.3K Mar  9 08:50 README.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tLA7i6-WI6W",
        "colab_type": "code",
        "outputId": "1c4d3386-341f-46aa-b446-7fe94e84cffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "%%bash\n",
        "python \"${TRAC_PATH}\"/src/generate_data.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "train\n",
            "./ENG/Sub-task A/train.tsv\n",
            "./ENG/Sub-task B/train.tsv\n",
            "./IBEN/Sub-task A/train.tsv\n",
            "./IBEN/Sub-task B/train.tsv\n",
            "./HIN/Sub-task A/train.tsv\n",
            "./HIN/Sub-task B/train.tsv\n",
            "dev\n",
            "./ENG/Sub-task A/dev.tsv\n",
            "./ENG/Sub-task B/dev.tsv\n",
            "./IBEN/Sub-task A/dev.tsv\n",
            "./IBEN/Sub-task B/dev.tsv\n",
            "./HIN/Sub-task A/dev.tsv\n",
            "./HIN/Sub-task B/dev.tsv\n",
            "test\n",
            "./ENG/Sub-task A/test.tsv\n",
            "./ENG/Sub-task B/test.tsv\n",
            "./IBEN/Sub-task A/test.tsv\n",
            "./IBEN/Sub-task B/test.tsv\n",
            "./HIN/Sub-task A/test.tsv\n",
            "./HIN/Sub-task B/test.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLHYsWUCTzQV",
        "colab_type": "code",
        "outputId": "9f632834-b675-4e05-b65f-7e87fbf3d6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "python \"${TRAC_PATH}\"/src/run_experiment_transformers_all.py --help"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: run_experiment_transformers_all.py [-h] --data_dir DATA_DIR\n",
            "                                          --model_type MODEL_TYPE\n",
            "                                          --model_name_or_path\n",
            "                                          MODEL_NAME_OR_PATH --task TASK\n",
            "                                          --lang LANG --output_dir OUTPUT_DIR\n",
            "                                          [--config_name CONFIG_NAME]\n",
            "                                          [--tokenizer_name TOKENIZER_NAME]\n",
            "                                          [--cache_dir CACHE_DIR]\n",
            "                                          [--max_seq_length MAX_SEQ_LENGTH]\n",
            "                                          [--do_train] [--do_eval]\n",
            "                                          [--evaluate_during_training]\n",
            "                                          [--do_lower_case]\n",
            "                                          [--train_batch_size TRAIN_BATCH_SIZE]\n",
            "                                          [--eval_batch_size EVAL_BATCH_SIZE]\n",
            "                                          [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                                          [--learning_rate LEARNING_RATE]\n",
            "                                          [--weight_decay WEIGHT_DECAY]\n",
            "                                          [--adam_epsilon ADAM_EPSILON]\n",
            "                                          [--max_grad_norm MAX_GRAD_NORM]\n",
            "                                          [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                                          [--max_steps MAX_STEPS]\n",
            "                                          [--warmup_steps WARMUP_STEPS]\n",
            "                                          [--logging_steps LOGGING_STEPS]\n",
            "                                          [--save_steps SAVE_STEPS]\n",
            "                                          [--eval_all_checkpoints] [--no_cuda]\n",
            "                                          [--overwrite_output_dir]\n",
            "                                          [--overwrite_cache] [--seed SEED]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n",
            "                        other data files) for the task.\n",
            "  --model_type MODEL_TYPE\n",
            "                        Model type selected in the list: bert, xlnet, xlm,\n",
            "                        roberta, distilbert, albert, xlmroberta, flaubert\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        Path to pre-trained model or shortcut name selected in\n",
            "                        the list: bert-base-uncased, bert-large-uncased, bert-\n",
            "                        base-cased, bert-large-cased, bert-base-multilingual-\n",
            "                        uncased, bert-base-multilingual-cased, bert-base-\n",
            "                        chinese, bert-base-german-cased, bert-large-uncased-\n",
            "                        whole-word-masking, bert-large-cased-whole-word-\n",
            "                        masking, bert-large-uncased-whole-word-masking-\n",
            "                        finetuned-squad, bert-large-cased-whole-word-masking-\n",
            "                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n",
            "                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n",
            "                        uncased, bert-base-japanese, bert-base-japanese-whole-\n",
            "                        word-masking, bert-base-japanese-char, bert-base-\n",
            "                        japanese-char-whole-word-masking, bert-base-finnish-\n",
            "                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n",
            "                        dutch-cased, xlnet-base-cased, xlnet-large-cased, xlm-\n",
            "                        mlm-en-2048, xlm-mlm-ende-1024, xlm-mlm-enfr-1024,\n",
            "                        xlm-mlm-enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n",
            "                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n",
            "                        xlm-mlm-17-1280, xlm-mlm-100-1280, roberta-base,\n",
            "                        roberta-large, roberta-large-mnli, distilroberta-base,\n",
            "                        roberta-base-openai-detector, roberta-large-openai-\n",
            "                        detector, distilbert-base-uncased, distilbert-base-\n",
            "                        uncased-distilled-squad, distilbert-base-cased,\n",
            "                        distilbert-base-cased-distilled-squad, distilbert-\n",
            "                        base-german-cased, distilbert-base-multilingual-cased,\n",
            "                        distilbert-base-uncased-finetuned-sst-2-english,\n",
            "                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n",
            "                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n",
            "                        albert-xlarge-v2, albert-xxlarge-v2, xlm-roberta-base,\n",
            "                        xlm-roberta-large, xlm-roberta-large-finetuned-\n",
            "                        conll02-dutch, xlm-roberta-large-finetuned-\n",
            "                        conll02-spanish, xlm-roberta-large-finetuned-\n",
            "                        conll03-english, xlm-roberta-large-finetuned-\n",
            "                        conll03-german, flaubert-small-cased, flaubert-base-\n",
            "                        uncased, flaubert-base-cased, flaubert-large-cased\n",
            "  --task TASK           The name of the task to train selected in the list:\n",
            "                        task_1, task_2, task_3\n",
            "  --lang LANG           The name of the lang to train selected in the list:\n",
            "                        EN, DE, HI\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pre-trained models\n",
            "                        downloaded from s3\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --do_train            Whether to run training.\n",
            "  --do_eval             Whether to run eval on the dev set.\n",
            "  --evaluate_during_training\n",
            "                        Rul evaluation during training at each logging step.\n",
            "  --do_lower_case       Set this flag if you are using an uncased model.\n",
            "  --train_batch_size TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for training.\n",
            "  --eval_batch_size EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for Adam.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight deay if we apply some.\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for Adam optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --eval_all_checkpoints\n",
            "                        Evaluate all checkpoints starting with the same prefix\n",
            "                        as model_name ending and ending with step number\n",
            "  --no_cuda             Avoid using CUDA when available\n",
            "  --overwrite_output_dir\n",
            "                        Overwrite the content of the output directory\n",
            "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
            "  --seed SEED           random seed for initialization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwd1ny1UpCm",
        "colab_type": "code",
        "outputId": "a3b66889-3b76-4a04-ec1c-371f508e8272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%bash\n",
        "TASK=\"Sub-task A\"\n",
        "LANG=\"ENG\"\n",
        "MODEL=\"xlmroberta\"\n",
        "#MODEL_NAME = \"xlm-roberta-base\"\n",
        "#--model_name_or_path xlm-roberta-base \\\n",
        "python \"${TRAC_PATH}\"/src/run_experiment_transformers_all.py \\\n",
        "--data_dir \"./${LANG}/${TASK}\" \\\n",
        "--model_type \"${MODEL}\" \\\n",
        "--model_name_or_path xlm-roberta-base \\\n",
        "--task \"${TASK}\" \\\n",
        "--lang ${LANG} \\\n",
        "--output_dir \"./${LANG}/${TASK}/output\" \\\n",
        "--cache_dir \"./${LANG}/${TASK}\" \\\n",
        "--num_train_epochs 5 \\\n",
        "--train_batch_size 32 \\\n",
        "--eval_batch_size 32 \\\n",
        "--save_steps 50 \\\n",
        "--logging_steps 5 \\\n",
        "--gradient_accumulation_steps 2 \\"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/gdrive/My Drive/TRAC2020-master//src/run_experiment_transformers_all.py\", line 745, in <module>\n",
            "    train_model(args)\n",
            "  File \"/content/gdrive/My Drive/TRAC2020-master//src/run_experiment_transformers_all.py\", line 710, in train_model\n",
            "    train_dataset = load_and_cache_examples(args, tokenizer, file_key=\"train\")\n",
            "  File \"/content/gdrive/My Drive/TRAC2020-master//src/run_experiment_transformers_all.py\", line 314, in load_and_cache_examples\n",
            "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
            "  File \"/content/gdrive/My Drive/TRAC2020-master//src/run_experiment_transformers_all.py\", line 314, in <listcomp>\n",
            "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
            "AttributeError: 'InputFeatures' object has no attribute 'label_id'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RyOxxqTfL0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2iXQ1dRVOUA",
        "colab_type": "code",
        "outputId": "4994457f-85b6-4278-af4e-d3e11959b490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls './EN/Sub-task A/train.tsv'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access './EN/Sub-task A/train.tsv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEz1mhMgVpgc",
        "colab_type": "code",
        "outputId": "8aec42fd-4494-4f6a-8ace-ed5a9f68d9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Mar  9 10:21:22 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.59       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9Yho6HV0DEe",
        "colab_type": "code",
        "outputId": "33a98bdc-7bd8-41eb-d217-082d0506eec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%bash\n",
        "TASK=\"Sub-task B\"\n",
        "LANG=\"ENG\"\n",
        "MODEL=\"bert-base-cased\"\n",
        "mkdir -p \"./${LANG}/${TASK}/${MODEL}\" \n",
        "mv \"./${LANG}/${TASK}/output/\"/* \"./${LANG}/${TASK}/${MODEL}/\"\n",
        "mv \"./${LANG}/${TASK}/${MODEL}\" \"./${LANG}/${TASK}/output/\"\n",
        "ls \"./${LANG}/${TASK}/output/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-cased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q1q462Z1wfI",
        "colab_type": "code",
        "outputId": "c11855f9-a1d0-4013-c2e0-e0221702b6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%%bash\n",
        "TASK=\"Sub-task B\"\n",
        "LANG=\"ENG\"\n",
        "MODEL=\"bert-base-cased\"\n",
        "tar -czvf \"./${LANG}/${TASK}/output/${MODEL}.tar.gz\" \"./${LANG}/${TASK}/output/${MODEL}\"/{*.tsv,*.json,events.*,model/config.json}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./ENG/Sub-task B/output/bert-base-cased/dev.tsv\n",
            "./ENG/Sub-task B/output/bert-base-cased/test.tsv\n",
            "./ENG/Sub-task B/output/bert-base-cased/train.tsv\n",
            "./ENG/Sub-task B/output/bert-base-cased/dev_results.json\n",
            "./ENG/Sub-task B/output/bert-base-cased/train_results.json\n",
            "./ENG/Sub-task B/output/bert-base-cased/events.out.tfevents.1583744522.3d02ea471dda\n",
            "./ENG/Sub-task B/output/bert-base-cased/model/config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTatpup7-2EL",
        "colab_type": "code",
        "outputId": "9d1507ec-c7c6-4f60-dee1-641c117e93c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "%%bash\n",
        "TASK=\"Sub-task B\"\n",
        "LANG=\"ENG\"\n",
        "MODEL=\"bert-base-cased\"\n",
        "tar -czvf \"./${LANG}/${TASK}/output/${MODEL}.tar.gz\" \"./${LANG}/${TASK}/output/${MODEL}\"/{*.tsv,*.json,events.*,model/config.json}\n",
        "# Upload to bashupload\n",
        "curl \"https://bashupload.com/${LANG}_${TASK}_${MODEL}.tar.gz\" --data-binary @\"./${LANG}/${TASK}/output/${MODEL}.tar.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./ENG/Sub-task B/output/bert-base-cased/dev.tsv\n",
            "./ENG/Sub-task B/output/bert-base-cased/test.tsv\n",
            "./ENG/Sub-task B/output/bert-base-cased/train.tsv\n",
            "./ENG/Sub-task B/output/bert-base-cased/dev_results.json\n",
            "./ENG/Sub-task B/output/bert-base-cased/train_results.json\n",
            "./ENG/Sub-task B/output/bert-base-cased/events.out.tfevents.1583744522.3d02ea471dda\n",
            "./ENG/Sub-task B/output/bert-base-cased/model/config.json\n",
            "\n",
            "Uploaded 1 file, 136246 bytes\n",
            "\n",
            "wget https://bashupload.com/KyOi0/eaxwF.gz\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  133k    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  133k    0    77  100  133k    119   205k --:--:-- --:--:-- --:--:--  205k\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFiCXecn_fRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a88e1de6-7979-4837-8a93-9d28461e3f75"
      },
      "source": [
        "from transformers import *\n",
        "ALL_MODELS1 = sum(\n",
        "    (\n",
        "        tuple(conf.pretrained_config_archive_map.keys())\n",
        "        for conf in (\n",
        "            BertConfig,\n",
        "            XLNetConfig,\n",
        "            XLMConfig,\n",
        "            RobertaConfig,\n",
        "            DistilBertConfig,\n",
        "            AlbertConfig,\n",
        "            XLMRobertaConfig,\n",
        "            FlaubertConfig,\n",
        "        )\n",
        "    ),\n",
        "    (),\n",
        ")\n",
        "ALL_MODELS1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bert-base-uncased',\n",
              " 'bert-large-uncased',\n",
              " 'bert-base-cased',\n",
              " 'bert-large-cased',\n",
              " 'bert-base-multilingual-uncased',\n",
              " 'bert-base-multilingual-cased',\n",
              " 'bert-base-chinese',\n",
              " 'bert-base-german-cased',\n",
              " 'bert-large-uncased-whole-word-masking',\n",
              " 'bert-large-cased-whole-word-masking',\n",
              " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
              " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
              " 'bert-base-cased-finetuned-mrpc',\n",
              " 'bert-base-german-dbmdz-cased',\n",
              " 'bert-base-german-dbmdz-uncased',\n",
              " 'bert-base-japanese',\n",
              " 'bert-base-japanese-whole-word-masking',\n",
              " 'bert-base-japanese-char',\n",
              " 'bert-base-japanese-char-whole-word-masking',\n",
              " 'bert-base-finnish-cased-v1',\n",
              " 'bert-base-finnish-uncased-v1',\n",
              " 'bert-base-dutch-cased',\n",
              " 'xlnet-base-cased',\n",
              " 'xlnet-large-cased',\n",
              " 'xlm-mlm-en-2048',\n",
              " 'xlm-mlm-ende-1024',\n",
              " 'xlm-mlm-enfr-1024',\n",
              " 'xlm-mlm-enro-1024',\n",
              " 'xlm-mlm-tlm-xnli15-1024',\n",
              " 'xlm-mlm-xnli15-1024',\n",
              " 'xlm-clm-enfr-1024',\n",
              " 'xlm-clm-ende-1024',\n",
              " 'xlm-mlm-17-1280',\n",
              " 'xlm-mlm-100-1280',\n",
              " 'roberta-base',\n",
              " 'roberta-large',\n",
              " 'roberta-large-mnli',\n",
              " 'distilroberta-base',\n",
              " 'roberta-base-openai-detector',\n",
              " 'roberta-large-openai-detector',\n",
              " 'distilbert-base-uncased',\n",
              " 'distilbert-base-uncased-distilled-squad',\n",
              " 'distilbert-base-cased',\n",
              " 'distilbert-base-cased-distilled-squad',\n",
              " 'distilbert-base-german-cased',\n",
              " 'distilbert-base-multilingual-cased',\n",
              " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
              " 'albert-base-v1',\n",
              " 'albert-large-v1',\n",
              " 'albert-xlarge-v1',\n",
              " 'albert-xxlarge-v1',\n",
              " 'albert-base-v2',\n",
              " 'albert-large-v2',\n",
              " 'albert-xlarge-v2',\n",
              " 'albert-xxlarge-v2',\n",
              " 'xlm-roberta-base',\n",
              " 'xlm-roberta-large',\n",
              " 'xlm-roberta-large-finetuned-conll02-dutch',\n",
              " 'xlm-roberta-large-finetuned-conll02-spanish',\n",
              " 'xlm-roberta-large-finetuned-conll03-english',\n",
              " 'xlm-roberta-large-finetuned-conll03-german',\n",
              " 'flaubert-small-cased',\n",
              " 'flaubert-base-uncased',\n",
              " 'flaubert-base-cased',\n",
              " 'flaubert-large-cased')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3IbPRxOzS5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2b8dd648-68d0-4de5-e94b-a8ae849c88e6"
      },
      "source": [
        "%%bash\n",
        "pwd\n",
        "ls\n",
        "cd ..\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "gdrive\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBUZXVUO1_E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}