{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TRAC_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuVTg8VLViNd",
        "colab_type": "code",
        "outputId": "eab0d8ed-4187-4daa-abe7-7ca4232456d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "%%bash\n",
        "pip install torch pytorch-transformers tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Collecting pytorch-transformers\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "Collecting tensorboardX\n",
            "  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "Collecting sacremoses\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.17.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.11.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.14.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (45.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py): started\n",
            "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=de9a808edbcc04f1ef2a8271cddb0f05ef50af99d44393ae68c11737b00fdbdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers, tensorboardX\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.38 sentencepiece-0.1.85 tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpgcK6edVyt7",
        "colab_type": "code",
        "outputId": "72087bc9-96ef-4d02-d094-f1aee0336b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKblDQmBV5ky",
        "colab_type": "code",
        "outputId": "7760c0b0-711c-4e9a-e9d4-9ca1cf8c5d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%env TRAC_PATH /content/gdrive/My Drive/TRAC2020/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: TRAC_PATH=/content/gdrive/My Drive/TRAC2020/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM-Kk2LHWD5J",
        "colab_type": "code",
        "outputId": "fd7c47af-c7a7-45d9-f624-d28efe001e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "%%bash \n",
        "echo \"${TRAC_PATH}\"\n",
        "ls -ltrh \"${TRAC_PATH}/data\"\n",
        "realpath \"${TRAC_PATH}\"\n",
        "ls -ltrh \"${TRAC_PATH}\"/data/**/*"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/TRAC2020/\n",
            "total 4.0K\n",
            "drwx------ 2 root root 4.0K Mar  5 10:20 raw\n",
            "/content/gdrive/My Drive/TRAC2020\n",
            "-rw------- 1 root root  321 Mar  8 16:08 /content/gdrive/My Drive/TRAC2020//data/raw/README.txt\n",
            "\n",
            "/content/gdrive/My Drive/TRAC2020//data/raw/iben:\n",
            "total 565K\n",
            "-rw------- 1 root root  82K Mar  8 16:08 trac2_iben_test.csv\n",
            "-rw------- 1 root root 389K Mar  8 18:08 trac2_iben_train.csv\n",
            "-rw------- 1 root root 1.3K Mar  8 18:08 README.txt\n",
            "-rw------- 1 root root  93K Mar  8 18:08 trac2_iben_dev.csv\n",
            "\n",
            "/content/gdrive/My Drive/TRAC2020//data/raw/hin:\n",
            "total 812K\n",
            "-rw------- 1 root root 174K Mar  8 16:08 trac2_hin_test.csv\n",
            "-rw------- 1 root root 1.3K Mar  8 18:08 README.txt\n",
            "-rw------- 1 root root 126K Mar  8 18:08 trac2_hin_dev.csv\n",
            "-rw------- 1 root root 511K Mar  8 18:08 trac2_hin_train.csv\n",
            "\n",
            "/content/gdrive/My Drive/TRAC2020//data/raw/eng:\n",
            "total 908K\n",
            "-rw------- 1 root root 281K Mar  8 16:08 trac2_eng_test.csv\n",
            "-rw------- 1 root root 1.3K Mar  8 18:08 README.txt\n",
            "-rw------- 1 root root 127K Mar  8 18:08 trac2_eng_dev.csv\n",
            "-rw------- 1 root root 499K Mar  8 18:08 trac2_eng_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tLA7i6-WI6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "55a36aeb-0fc3-40d3-e1ca-70a2386fc40e"
      },
      "source": [
        "%%bash\n",
        "python \"${TRAC_PATH}\"/src/generate_data.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "train\n",
            "./ENG/Sub-task A/train.tsv\n",
            "./ENG/Sub-task B/train.tsv\n",
            "./IBEN/Sub-task A/train.tsv\n",
            "./IBEN/Sub-task B/train.tsv\n",
            "./HIN/Sub-task A/train.tsv\n",
            "./HIN/Sub-task B/train.tsv\n",
            "dev\n",
            "./ENG/Sub-task A/dev.tsv\n",
            "./ENG/Sub-task B/dev.tsv\n",
            "./IBEN/Sub-task A/dev.tsv\n",
            "./IBEN/Sub-task B/dev.tsv\n",
            "./HIN/Sub-task A/dev.tsv\n",
            "./HIN/Sub-task B/dev.tsv\n",
            "test\n",
            "./ENG/Sub-task A/test.tsv\n",
            "./ENG/Sub-task B/test.tsv\n",
            "./IBEN/Sub-task A/test.tsv\n",
            "./IBEN/Sub-task B/test.tsv\n",
            "./HIN/Sub-task A/test.tsv\n",
            "./HIN/Sub-task B/test.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLHYsWUCTzQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e9abfd0-5edf-442b-f925-4d26138ee34d"
      },
      "source": [
        "%%bash\n",
        "python \"${TRAC_PATH}\"/src/run_experiment.py --help"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: run_experiment.py [-h] --data_dir DATA_DIR --bert_model_type\n",
            "                         BERT_MODEL_TYPE --task TASK --lang LANG --output_dir\n",
            "                         OUTPUT_DIR [--cache_dir CACHE_DIR]\n",
            "                         [--max_seq_length MAX_SEQ_LENGTH] [--do_train]\n",
            "                         [--do_eval] [--evaluate_during_training]\n",
            "                         [--do_lower_case]\n",
            "                         [--train_batch_size TRAIN_BATCH_SIZE]\n",
            "                         [--eval_batch_size EVAL_BATCH_SIZE]\n",
            "                         [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                         [--learning_rate LEARNING_RATE]\n",
            "                         [--weight_decay WEIGHT_DECAY]\n",
            "                         [--adam_epsilon ADAM_EPSILON]\n",
            "                         [--max_grad_norm MAX_GRAD_NORM]\n",
            "                         [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                         [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
            "                         [--logging_steps LOGGING_STEPS]\n",
            "                         [--save_steps SAVE_STEPS] [--eval_all_checkpoints]\n",
            "                         [--no_cuda] [--overwrite_output_dir]\n",
            "                         [--overwrite_cache] [--seed SEED]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n",
            "                        other data files) for the task.\n",
            "  --bert_model_type BERT_MODEL_TYPE\n",
            "                        Bert Model type selected in the list:\n",
            "  --task TASK           The name of the task to train selected in the list:\n",
            "                        task_1, task_2, task_3\n",
            "  --lang LANG           The name of the lang to train selected in the list:\n",
            "                        EN, DE, HI\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pre-trained models\n",
            "                        downloaded from s3\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --do_train            Whether to run training.\n",
            "  --do_eval             Whether to run eval on the dev set.\n",
            "  --evaluate_during_training\n",
            "                        Rul evaluation during training at each logging step.\n",
            "  --do_lower_case       Set this flag if you are using an uncased model.\n",
            "  --train_batch_size TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for training.\n",
            "  --eval_batch_size EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for Adam.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight deay if we apply some.\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for Adam optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --eval_all_checkpoints\n",
            "                        Evaluate all checkpoints starting with the same prefix\n",
            "                        as model_name ending and ending with step number\n",
            "  --no_cuda             Avoid using CUDA when available\n",
            "  --overwrite_output_dir\n",
            "                        Overwrite the content of the output directory\n",
            "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
            "  --seed SEED           random seed for initialization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwd1ny1UpCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "TASK=\"Sub-task A\"\n",
        "LANG=\"ENG\"\n",
        "MODEL=\"bert-base-cased\"\n",
        "python \"${TRAC_PATH}\"/src/run_experiment.py \\\n",
        "--data_dir \"./${LANG}/${TASK}\" \\\n",
        "--bert_model_type \"${MODEL}\" \\\n",
        "--task \"${TASK}\" \\\n",
        "--lang ${LANG} \\\n",
        "--output_dir \"./${LANG}/${TASK}/output\" \\\n",
        "--cache_dir \"./${LANG}/${TASK}\" \\\n",
        "--num_train_epochs 5 \\\n",
        "--train_batch_size 32 \\\n",
        "--eval_batch_size 32 \\\n",
        "--save_steps 50 \\\n",
        "--logging_steps 5 \\\n",
        "--gradient_accumulation_steps 2 \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2iXQ1dRVOUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4994457f-85b6-4278-af4e-d3e11959b490"
      },
      "source": [
        "! ls './EN/Sub-task A/train.tsv'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access './EN/Sub-task A/train.tsv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEz1mhMgVpgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}